{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework you will be using `pytorch` and `torchvision` library for neural networks and datasets. You can install them with `pip install torch torchvision`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Principal Component Analysis\n",
    "This problem will guide you through the principal component analysis. You will be using a classical dataset, the MNIST hand written digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/home/sniradi/Documents/ucsd/152/env_152/lib/python3.6/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/sniradi/Documents/ucsd/152/env_152/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "shapes: (60000, 28, 28) (60000,)\n",
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = MNIST('.', download=True)\n",
    "data = mnist.train_data.numpy()\n",
    "labels = mnist.train_labels.numpy()\n",
    "print('shapes:', data.shape, labels.shape)\n",
    "plt.imshow(data[0])\n",
    "print('label:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 Familiarize yourself with the data [5pt]\n",
    "For this task, you will be using the torchvision package that provides the MNIST dataset. For each digit class(0-9), plot 1 image from the class and store those 10 images for each digit class in the array `digit_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "for img, label in zip(data, labels):\n",
    "    if label in digits and digit_count[label] < 5:\n",
    "        rand_clusters[pred].append(patch)\n",
    "        cluster_count[pred] += 1\n",
    "    if sum([val for key, val in cluster_count.items()]) >= 50:\n",
    "        break\n",
    "        \n",
    "fig = plt.figure(figsize = (16, 16))\n",
    "\n",
    "count = 1\n",
    "rows, cols = 10, 5\n",
    "\n",
    "for key in rand_clusters:\n",
    "    for image in enumerate(rand_clusters[key]):\n",
    "        fig.add_subplot(rows, cols, count)\n",
    "        plt.imshow(image[1].reshape(16, 16, 3) / 255)\n",
    "        count += 1\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_count = np.ones((10)) * -1\n",
    "digit_images = np.zeros([10, 28, 28])\n",
    "### YOUR CODE HERE\n",
    "for image, label in zip(data, labels):\n",
    "    if np.sum(digit_count) < 10:\n",
    "        if digit_count[label] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            digit_images[label] = image\n",
    "            digit_count[label] = 1\n",
    "    else:\n",
    "        break\n",
    "digit_images\n",
    "# fig = plt.figure()\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 PCA\n",
    "The following questions will guide you through the PCA algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.1 Centering the data [5pt]\n",
    "For each image, flatten it to a 1-D vector. To perform PCA on the dataset, we first move the data points so they have 0 mean on each dimension. Store the centered data in variable `data_centered` and the mean of each dimension in variable `data_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centered = None\n",
    "data_mean = None\n",
    "### YOUR CODE HERE\n",
    "data_mean = np.mean(data, axis=0)\n",
    "data_centered = data - data_mean \n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.2 Compute the covariance matrix of the data [5pt]\n",
    "You need to store the covariance matrix of the data in variable `data_covmat`. You may **not** use numpy.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covmat = None\n",
    "### YOUR CODE HERE\n",
    "data_covmat = (1 / data.shape[0]) * np.dot(X.T, X)\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.3 Compute the eigenvalues of the covariance matrix [5pt]\n",
    "You need to store the eigenvalues of the covariance matrix in variable `covmat_eig`, sorted in descending order. Then you need to plot the eigenvalues with `plt.plot`. You can use any numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat_eig = None\n",
    "### YOUR CODE HERE\n",
    "covmat_eig, _ = np.linalg.eigh(data_covmat)\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.4 Project data onto the first 2 principal components [5pt]\n",
    "Now you need to project the centered data on the 2D space formed by the eigenvectors corresponding to the 2 largest eigenvalues. Create a 2D scatter plot where you need to assign a unique color to each digit class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.5 Unproject data back to high dimensions [10pt]\n",
    "For this question, you need to project the 10 images you plotted in **1.1** on the first 2 principal components, and then unproject the \"compressed\" 2-D representations back to the original space. Plot the \"compressed\" digit (the reconstructed digit). Do they look similar to the original images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.6 Choose a better low dimension space. [5pt]\n",
    "Do the previous problem with more dimensions (e.g. 3, 5, 10, 20, 50, 100). You only need to show results for one of them. Answer the following questinos. How many dimensions are required to represent the digits reasonably well? How are your results related to **question 1.2.3**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Your explanation)\n",
    "<br></br><br></br><br></br><br></br><br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 Harris Corner and PCA [10pt]\n",
    "Recall Harris corner detector algorithm:\n",
    "1. Compute $x$ and $y$ derivatives ($I_x, I_y$) of an image\n",
    "2. Compute products of derivatives ($I_x^2, I_y^2, I_{xy}$) at each pixel\n",
    "3. Compute matrix $M$ at each pixel, where\n",
    "$$\n",
    "M(x_0,y_0) = \\sum_{x,y} w(x-x_0,y-y_0)\n",
    "    \\begin{bmatrix}\n",
    "        I_{x}^2 & I_{x}I_{y} \\\\\n",
    "        I_{x}I_{y} & I_{y}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "Here, we set weight $w(x,y)$ to be a box filter of size $3 \\times 3$ (the box is placed centered at $(x_0, y_0)$).\n",
    "\n",
    "In this problem, you need to show that Harris Corner detector is really just principal component analysis in the gradient space. Your explanation should answer the following quesions.\n",
    "1. As we know, PCA is performed on data points. What are the data points in Harris corner detector when we think of it as a PCA?\n",
    "2. What is the covariance matrix used in Harris corner detector and why it is a covariance matrix?\n",
    "3. What are the principal components in Harris corner detector?\n",
    "4. Briefly explain how principal components imply \"cornerness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Your proof here)\n",
    "<br></br><br></br><br></br><br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 KNN, Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='.', train=True, transform=transforms.ToTensor, download=True)\n",
    "test_dataset = MNIST('.', train=False, transform=transforms.ToTensor())\n",
    "train_X = train_dataset.data.numpy()  # training data, uint8 type to reduce memory and comparison cost\n",
    "train_y = train_dataset.targets.numpy()  # training label\n",
    "test_X = test_dataset.data.numpy() # testing data, uint8 to reduce memory and comparison cost\n",
    "test_y = test_dataset.targets.numpy()  # testing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "test_X = test_X.reshape((test_X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 K-Nearest Neighbor [10pt]\n",
    "In this problem you will be implementing the KNN classifier. Fill in the functions in the starter code below. You are are allowed to use `scipy.spatial.KDTree` and `scipy.stats.mode` (in case of a tie, pick any one). Please avoid `sklearn.neighbors.KDTree` as it appears extremely slow. You are **not** allowed to use a library KNN function that directly solves the problem.\n",
    "\n",
    "If you do not know what a KD-tree is, please read the documentation for `scipy.spatial.KDTree` to understand how you can use it.\n",
    "\n",
    "Note: if you run into memory issues or neighbor queries run for more than 10 minutes, you are allowed to reduce the data size, and explain what you have done to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, num_neighbors):\n",
    "        \"\"\"\n",
    "        construct the classifier\n",
    "        Args:\n",
    "            num_centers: number of neighbors\n",
    "        \"\"\"\n",
    "        ### YOU CODE HERE\n",
    "        ### END OF CODE\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        train KNN classifier\n",
    "        Args:\n",
    "            X: training data, numpy array with shape (Nxk) where N is number of data points, k is number of features\n",
    "            y: training labels, numpy array with shape (N)\n",
    "        \"\"\"\n",
    "        ### YOU CODE HERE\n",
    "        ### END OF CODE\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict labels\n",
    "        Args:\n",
    "            X: testing data, numpy array with shape (Mxk) where M is number of data points, k is number of features\n",
    "        Return:\n",
    "            y: predicted labels, numpy array with shape (N)\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        ### YOU CODE HERE\n",
    "        ### END OF CODE\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "knn = KNNClassifier(3).fit(train_X, train_y)\n",
    "pred_y = knn.predict(test_X)\n",
    "print('KNN accuracy:', accuracy_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 Softmax Regression\n",
    "In this problm, you will be implementing the softmax regression(multi-class logistic regression). Here is a brief recap of several important concepts. In the following explanation, I will use $x$ for data vector, $y'$ for ground truth label, and $y$ for predicted label.\n",
    "\n",
    "Suppose we have a problem where we need to classify data points into $m$ classes. \n",
    "\n",
    "1. Softmax function $S$ normalize a vector to have sum 1. (it turns any vector into a probability distribution)\n",
    "$$S(x) = [\\frac{e^{x_1}}{\\sum_{j=1}^m{e^{x_j}}}, \\frac{e^{x_2}}{\\sum_{j=1}^m{e^{x_j}}},...,\\frac{e^{x_m}}{\\sum_{j=1}^m{e^{x_j}}}]$$\n",
    "\n",
    "2. Cross entropy loss $J$ is the multiclass logistic regression loss.\n",
    "$$J(y', y)=-\\sum_{i=1}^m y_i'\\log{y_i}$$ where $y'$ is the one-hot ground truth label and $y$ is the predicted label distribution.\n",
    "\n",
    "3. Softmax regression is the following optimization problem.\n",
    "$$\\min_{W,b} \\sum_{(X,y')\\in \\text{\\{training set\\}}}J(y',S(Wx+b))$$\n",
    "where $W$ has shape $(m \\times k)$ where $k$ is the number of features in a data point; $b$ is a $m$ dimensional vector.\n",
    "\n",
    "4. This objective is optimized with gradient descent. Let\n",
    "$$L = \\sum_{(x,y')\\in \\text{\\{training set\\}}}J(y',S(Wx+b))$$\n",
    "Update $W$ and $b$ with $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2.1 Compute the gradients [10pt]\n",
    "In this question, you need to do the following:\n",
    "1. Compute the gradient $\\frac{\\partial J}{\\partial y}$. i.e. compute $$\\frac{\\partial J}{\\partial y_i}$$\n",
    "Express it in terms of $y_i'$ and $y_i$.\n",
    "2. Let $u=Wx+b$, $y_i=S_i(u_j)$ Compute $$\\frac{\\partial y_i}{\\partial u_j}$$ Express it in terms of $y_i, y_j$ and $\\delta_{ij}$, where \n",
    "$$\\delta_{ij}=\\begin{cases}1 & i=j \\\\ 0 & i\\neq j\\end{cases}$$\n",
    "3. Compute $$\\frac{\\partial J}{\\partial W_{jk}} \\text{ and } \\frac{\\partial J}{\\partial b_j}$$\n",
    "Express them in terms of $y_j, y_j', x_k$. Explain your results in an intuitive way. Hint: the results should have a very simple form that makes sense intuitively.\n",
    "4. Compute $$\\frac{\\partial J}{\\partial W}$$ in the matrix form. It should be a matrix with the same shape as $W$, and entry $jk$ is $\\frac{\\partial J}{\\partial W_{jk}}$. Similarly, compute $$\\frac{\\partial J}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Your proof here)\n",
    "<br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2.2 Stochastic Gradient Descent [10pt]\n",
    "In gradient descent algorithm, we update $W$ and $b$ with $\\partial L / \\partial W$ and $\\partial L / \\partial b$. However, this requires the gradient w.r.t. the whole dataset. Computing such gradient is very slow. Instead, we can update the weights with per-data gradient. This is known as the SGD algorithm, which runs much faster. You need to take the following steps.\n",
    "1. Implement softmax function $S$. We need to take special care in this function since $e^x$ tends to overflow easily. However, we observe that $S(x) = S(x-m)$ for any constant vector $m$. We can stabilize softmax using $S(x) = S(x-\\max(x))$.\n",
    "2. Implement function `J`(loss) and `dJ`(loss gradient). Note: `J` is not required to run the algorithm, but you may want to implement it for debug purposes.\n",
    "3. Implement the SGD algorithm.\n",
    "4. Run the algorithm for 20 epochs (each epoch iterates the whole data set once) with learning rate `1e-3` and report accuracy on test set. You may use `sklearn.metrics.accuracy_score`. You need to achieve accuracy > 90%. You are allowed to experiment with different epoch numbers and learning rates (even learning rate decay) to achieve this accuracy, but they are not required.\n",
    "\n",
    "You may use print (or progress bar packages) to track the training progress since it might take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='.', train=True, transform=transforms.ToTensor, download=True)\n",
    "test_dataset = MNIST('.', train=False, transform=transforms.ToTensor())\n",
    "train_X = train_dataset.data.numpy() / 255.  # normalize data to 0-1\n",
    "train_y = train_dataset.targets.numpy()  # training label\n",
    "test_X = test_dataset.data.numpy() / 255.  # normalize data to 0-1\n",
    "test_y = test_dataset.targets.numpy()  # testing label\n",
    "train_X = train_X.reshape((train_X.shape[0], -1))  # flatten the image\n",
    "test_X = test_X.reshape((test_X.shape[0], -1))  # flatten the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax function\n",
    "    Args:\n",
    "        x: a 1-d numpy array\n",
    "    Return:\n",
    "        results of softmax(x)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e/np.sum(e)\n",
    "    ### END OF CODE\n",
    "    \n",
    "def J(W, b, y_true, x):\n",
    "    \"\"\"\n",
    "    Softmax Loss function\n",
    "    Args:\n",
    "        W: weights (num_classes x num_features)\n",
    "        b: bias (num_classes)\n",
    "        y_true: ground truth 1-hot label (num_classes)\n",
    "        x: input data\n",
    "    Return:\n",
    "        J(y', y)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    z = np.dot(W, x) + b\n",
    "    y_p = softmax(out)\n",
    "    return np.sum(-y_p * np.log(z))\n",
    "    ### END OF CODE\n",
    "\n",
    "def dJ(W, b, y_true, x):\n",
    "    \"\"\"\n",
    "    Softmax Loss gradient\n",
    "    Args:\n",
    "        W: weights (num_classes x num_features)\n",
    "        b: bias (num_features)\n",
    "        y_true: ground truth 1-hot label (num_classes)\n",
    "        x: input data (num_features)\n",
    "    Return:\n",
    "        (dW, db): gradient w.r.t. W and b\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    loss = J(W, b, y_true, x)\n",
    "    db\n",
    "    dW = \n",
    "    ### END OF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(f, df, Xs, ys, n_classes=10, lr=1e-3, max_epoch=20):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f: function to optimize\n",
    "        df: the gradient of the function\n",
    "        Xs: input data, numpy array with shape (num_data x num_features)\n",
    "        ys: true label, numpy array with shape (num_data x num_classes)\n",
    "        lr: learning rate\n",
    "        max_epoch: maximum epochs to run SGD\n",
    "    Return:\n",
    "        optimal weights and biases\n",
    "    \"\"\"\n",
    "    N, m = Xs.shape\n",
    "    W = np.random.rand(n_classes, m) - 0.5  # you do not need to change random initialization\n",
    "    b = np.random.rand(n_classes) - 0.5\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### END OF CODE\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_onehot = np.zeros((train_y.shape[0], 10))\n",
    "train_y_onehot[np.arange(len(train_y)), train_y] = 1\n",
    "W, b = SGD(J, dJ, train_X, train_y_onehot, 10, max_epoch=20)\n",
    "accuracy_score(test_y, np.argmax(test_X @ W.T + b, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Convolutional Neural Networks\n",
    "This question requires you to use the PyTorch framework for neural network training. You will not need GPU to train the networks for this problem.\n",
    "\n",
    "The following is a code sample for training a simple multi-layer perceptron neural network using PyTorch. Running it should give you about 98% testing accuracy.\n",
    "\n",
    "Since network training takes long, I recommend installing the tqdm package for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = MNIST('.', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"init function builds the required layers\"\"\"\n",
    "        super(MLP, self).__init__()  # This line is always required\n",
    "        # Hidden layer\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # output layer\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward function describes how input tensor is transformed to output tensor\"\"\"\n",
    "        # flatten the input from (Nx1x28x28) to (Nx784)\n",
    "        torch.flatten(x, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        # Note we do not need softmax layer, since this layer is included in the CrossEntropyLoss provided by torch\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(784, 1024, 10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'lr': 5e-4,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), opts['lr'])  # Adam is a much better optimizer compared to SGD\n",
    "criterion = torch.nn.CrossEntropyLoss()  # loss function\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=opts['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=opts['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(opts['epochs']):\n",
    "    train_loss = []\n",
    "    for i, (data, labels) in tqdm_notebook(enumerate(train_loader), total=len(train_loader)):\n",
    "        # reshape data\n",
    "        data = data.reshape([-1, 784])\n",
    "        # pass data through network\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()  # Important! Otherwise the optimizer will accumulate gradients from previous runs!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for i, (data, labels) in enumerate(test_loader):\n",
    "        # reshape data\n",
    "        data = data.reshape([-1, 784])\n",
    "        # pass data through network\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss.append(loss.item())\n",
    "        test_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
    "    print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format(epoch, np.mean(train_loss), np.mean(test_loss), np.mean(test_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 Implementing CNN [15pt]\n",
    "You need to implement a convolutional neural network for the same task as above. You may find the PyTorch documentation helpful. https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "We provide a working network structure below. You can adjust the network size and training options for better performance, but a correct implementation of the provided network should give you the required accuracy. For convolutional layers, (conv MxM, N) means the layer has kernel size $M$ by $M$ and $N$ output channels; for pooling layers, (maxpool MxM) means doing max pooling with kernel size $M$ by $M$.\n",
    "\n",
    "(conv 5x5, 32) -> (relu) -> (maxpool 2x2) -> (conv 5x5, 64) -> (relu) -> (maxpool 2x2) -> (flatten) -> (linear 10) -> (output)\n",
    "\n",
    "For full score, you need to achieve 99% testing accuracy. Also, plot the hand-written digits that your network got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\n",
    "        init convolution and activation layers\n",
    "        Args:\n",
    "            input_size: (1,28,28)\n",
    "            num_classes: 10\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__() \n",
    "        ### YOUR CODE HERE\n",
    "        ### END OF CODE\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward function describes how input tensor is transformed to output tensor\n",
    "        Args:\n",
    "            x: (Nx1x28x28) tensor\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END OF CODE\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN((1, 28, 28), 10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You may (and should) change these\n",
    "opts = {\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "### if you cannot get 99% with SGD, Adam optimizer can help you\n",
    "optimizer = torch.optim.Adam(model.parameters(), opts['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # loss function\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=opts['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=opts['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(opts['epochs']):\n",
    "    train_loss = []\n",
    "    for i, (data, labels) in tqdm_notebook(enumerate(train_loader), total=len(train_loader)):\n",
    "        # pass data through network\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for i, (data, labels) in enumerate(test_loader):\n",
    "        # pass data through network\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss.append(loss.item())\n",
    "        test_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
    "    print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format(epoch, np.mean(train_loss), np.mean(test_loss), np.mean(test_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget plotting the digits that the network got wrong.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 Kernel weights visualization [5pt]\n",
    "For this question, you need to visualize the kernel weights for your first convolutional layer. Suppose you have 5x5 kernels with 32 output channels. You will plot 32 5x5 images.\n",
    "\n",
    "hint: You might need to look at PyTorch documentation (or play with the PyTorch model) to figure out how to get the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### END OF CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
